{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802e3962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch test:Ture,device name:NVIDIA GeForce GTX 1080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sever_162/anaconda3/envs/pytroch_bert/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda_ava = \"Ture\" if torch.cuda.is_available() else \"False\"\n",
    "cuda_device = torch.cuda.get_device_name()\n",
    "print(\"pytorch test:\"+ cuda_ava +\",device name:\"+cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29f618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06dc0efa",
   "metadata": {},
   "source": [
    "BERT 輸入格式\n",
    "words = [self.CLS_TOKEN] + words + [self.SEP_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e57036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'good', 'time', ',', 'thank', 'you', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load bert model over\n"
     ]
    }
   ],
   "source": [
    "BERT_PATH = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "print(tokenizer.tokenize('I have a good time, thank you.'))\n",
    "bert = BertModel.from_pretrained(BERT_PATH)\n",
    "print('load bert model over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b423a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,   146,  1209,  2824,  2508, 26173,  3568,   102,     0,     0,\n",
      "             0,     0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "example_text = 'I will watch Memento tonight'\n",
    "bert_input = tokenizer(example_text,padding='max_length', \n",
    "                       max_length = 12, \n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\")\n",
    "# ------- bert_input ------\n",
    "print(bert_input['input_ids']) #tokens_tensor\n",
    "print(bert_input['token_type_ids']) #segments_tensor\n",
    "print(bert_input['attention_mask']) #masks_tensor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ae2d21a",
   "metadata": {},
   "source": [
    "下面是對上面BertTokenizer參數的解釋：\n",
    "\n",
    "padding：將每個 sequence 填充到指定的最大長度。\n",
    "\n",
    "max_length: 每個 sequence 的最大長度。本示例中我們使用 10，但對於本文實際數據集，我們將使用 512，這是 BERT 允許的 sequence 的最大長度。\n",
    "\n",
    "truncation：如果爲 True，則每個序列中超過最大長度的標記將被截斷。\n",
    "\n",
    "return_tensors：將返回的張量類型。由於我們使用的是 Pytorch，所以我們使用pt；如果你使用 Tensorflow，那麼你需要使用tf。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9016dea",
   "metadata": {},
   "source": [
    "第一行是 input_ids，它是每個 token 的 id 表示。實際上可以將這些輸入 id 解碼爲實際的 token\n",
    "BertTokenizer負責輸入文本的所有必要轉換，爲 BERT 模型的輸入做好準備。它會自動添加 [CLS]、[SEP] 和 [PAD] token。由於我們指定最大長度爲 10，所以最後只有兩個 [PAD] token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99e922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I will watch Memento tonight [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "example_text_2 = tokenizer.decode(bert_input.input_ids[0])\n",
    "print(example_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b37ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        #liner layer\n",
    "        self.out = nn.linear(768,1)\n",
    "        \n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        _,output_bert = self.bert_model(ids,attention_mask = mask, token_type_ids= token_type_ids, return_dict=False)\n",
    "        \n",
    "        out= self.out(output_bert)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = BERT()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b507a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytroch_bert]",
   "language": "python",
   "name": "conda-env-pytroch_bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
